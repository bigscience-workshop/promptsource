dataset: asset
subset: ratings
templates:
  09b2a13b-cba6-4473-8a46-3fa24be71ce2: !Template
    answer_choices: null
    id: 09b2a13b-cba6-4473-8a46-3fa24be71ce2
    jinja: "{% set questions= [ \"Does the second sentence better convey the information?\"\
      ,  \"Is the second sentence more fluent?\", \"Is the second sentence easier\
      \ to understand?\"] %}\n\nFirst sentence: {{original}}\n\nSecond sentence: {{simplification}}\n\
      \n{{questions[aspect]}} \n\n|||\n\n{% if rating > 50 %}\n    Yes\n{% else %}\n\
      \    No\n{% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: asset_ratings1
    reference: Taking questions from the original paper, we use rating to establish
      a binary classification problem.
  47142040-4121-4144-98b9-61cb5cbb1313: !Template
    answer_choices: null
    id: 47142040-4121-4144-98b9-61cb5cbb1313
    jinja: 'First sentence: {{original}}


      Second sentence: {{simplification}}


      I am scoring these simplification exercises. How easier to read is the second
      sentence on a scale from 0 (harder to read) to 100 (easier to read)?


      |||


      {{rating}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: true
    name: asset_ratings3
    reference: Prompt model to rate how simplified the sentence is in the general
      sense, instead of an particular aspect.
  d2bed959-29ab-4962-a106-dc91c00f3f03: !Template
    answer_choices: null
    id: d2bed959-29ab-4962-a106-dc91c00f3f03
    jinja: "{% set statements= [ \"the second sentence expresses the underlying meaning\
      \ the best.\",  \"the second sentence is more fluent.\", \"the second sentence\
      \ is easier to read and understand.\"] %}\n\nFirst sentence: {{original}}\n\n\
      Second sentence: {{simplification}}\n\nRate the following statement from 0 (strongly\
      \ disagree) to 100 (strongly agree): {{statements[aspect]}} \n\n|||\n\n{{rating}}"
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: true
    name: asset_ratings2
    reference: Require the model to output the rating
