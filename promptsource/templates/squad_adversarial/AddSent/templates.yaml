dataset: squad_adversarial
subset: AddSent
templates:
  048c2159-2c8c-40e2-90f7-18c9623381ba: !Template
    answer_choices: null
    id: 048c2159-2c8c-40e2-90f7-18c9623381ba
    jinja: 'Generate a possible question for the following short passage:


      {{context}} |||

      {{question}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: possible_qn
    reference: ''
  08fb6eac-6321-4b25-8578-14a799a103ed: !Template
    answer_choices: null
    id: 08fb6eac-6321-4b25-8578-14a799a103ed
    jinja: 'After reading the following paragraph, please answer this question: {{question}}


      {{context}}


      |||

      {{answers[''text''] | most_frequent | choice}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: true
    name: after
    reference: ''
  1f2c2108-441a-4b3c-a5c8-8ece28edb6e1: !Template
    answer_choices: null
    id: 1f2c2108-441a-4b3c-a5c8-8ece28edb6e1
    jinja: 'At what character does the text "{{answers["text"][0]}}" start in the
      following paragraph?


      {{context}}


      |||

      {{answers["answer_start"][0]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: find text
    reference: ''
  279e4019-8d67-498d-8832-a7905bc0c68d: !Template
    answer_choices: null
    id: 279e4019-8d67-498d-8832-a7905bc0c68d
    jinja: 'Use the following non-answers to generate a possible short passage-question
      pair:

      {{answers["text"]|join('', '')}} |||

      {{context}}

      {{question}}

      '
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: answers_question
    reference: ''
  44df6bac-bffa-4e46-b2d4-f3eb5b43cefa: !Template
    answer_choices: null
    id: 44df6bac-bffa-4e46-b2d4-f3eb5b43cefa
    jinja: 'Generate a title for the following short passage:


      {{context}} |||

      {{title|replace("_"," ")}}

      '
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: title
    reference: ''
  60ae905d-d5fa-4f60-bbcb-acb8d0ec2cf1: !Template
    answer_choices: null
    id: 60ae905d-d5fa-4f60-bbcb-acb8d0ec2cf1
    jinja: "Q: {{question}}\n\nA: \n|||\n{{answers['text'] | most_frequent | choice}}"
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: cbqa qa
    reference: ''
  6118ec43-d051-4599-b24f-8779f66b9ad6: !Template
    answer_choices: null
    id: 6118ec43-d051-4599-b24f-8779f66b9ad6
    jinja: '{{question}}


      |||


      {{answers[''text''] | most_frequent | choice}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: cbqa
    reference: ''
  754e8bad-454f-4ae3-9747-299506955569: !Template
    answer_choices: null
    id: 754e8bad-454f-4ae3-9747-299506955569
    jinja: 'Please come up with a good question to test reading comprehension about
      the following paragraph:


      {{context}}


      |||


      {{question}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: null
    name: generate question
    reference: ''
  7ff4bc14-08d4-47c1-9cd3-b7473d6505e7: !Template
    answer_choices: null
    id: 7ff4bc14-08d4-47c1-9cd3-b7473d6505e7
    jinja: 'For the following passage-question pair, list all possible wrong answers
      (pitfalls) test-takers may choose:


      {{context}}

      {{question}} |||

      {{answers["text"]|join(", ")}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: true
    name: possible_pitfalls
    reference: ''
  88b952a3-3784-43bb-a463-4a34478785d5: !Template
    answer_choices: null
    id: 88b952a3-3784-43bb-a463-4a34478785d5
    jinja: '{{["Question", "Problem"]  | choice}} {{range(1, 12) | choice}}: {{question}}


      Hint: {{context}}


      |||

      {{answers["text"] | most_frequent | choice}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: question/hint
    reference: ''
  8bcc0d77-6925-4fa1-b8cc-e6da3b272197: !Template
    answer_choices: null
    id: 8bcc0d77-6925-4fa1-b8cc-e6da3b272197
    jinja: "Question: {{question}}\n\nAnswer: \n|||\n{{answers['text'] | most_frequent\
      \ | choice}}"
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: cbqa question answer
    reference: ''
  a99d7cf5-d723-4c7a-b843-e2b8a476754d: !Template
    answer_choices: null
    id: a99d7cf5-d723-4c7a-b843-e2b8a476754d
    jinja: 'I''ve always wondered: {{question}}


      I searched Wikipedia and this is what I found. What''s the answer?


      {{context}}


      |||

      {{answers[''text''] | most_frequent | choice}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: wondered
    reference: ''
  a9d70ff7-8080-4eaa-9be2-1b67fe9b44f4: !Template
    answer_choices: null
    id: a9d70ff7-8080-4eaa-9be2-1b67fe9b44f4
    jinja: 'I''m working on the final exam for my class and am trying to figure out
      the answer to the question "{{question}}" I found the following info on Wikipedia
      and I think it has the answer. Can you tell me the answer?


      {{context}}


      |||

      {{answers[''text''] | most_frequent | choice}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: exam
    reference: ''
  f086fa63-6ca2-48d2-857d-179ab88fce48: !Template
    answer_choices: null
    id: f086fa63-6ca2-48d2-857d-179ab88fce48
    jinja: 'I''m creating a final exam for my reading class. Can you please come up
      with a good question to quiz how well students have read the following text
      snippet?


      {{context}}


      |||


      {{question}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: null
    name: exam creation help
    reference: ''
  f9b51e3b-a41a-47a5-b929-76a1e0efd430: !Template
    answer_choices: null
    id: f9b51e3b-a41a-47a5-b929-76a1e0efd430
    jinja: 'Count the characters up until "{{answers["text"][0]}}" appears in the
      following chunk of text.


      {{context}}


      |||


      {{answers["answer_start"][0]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: null
    name: count letters
    reference: ''
  fb81ba4d-341a-43f0-a94f-fa7e350d10c0: !Template
    answer_choices: null
    id: fb81ba4d-341a-43f0-a94f-fa7e350d10c0
    jinja: 'List all possible non-answers that have a lot of words in common with
      the following context-question pair:


      {{context}}

      {{question}} |||

      {{answers["text"]|join('', '')}}

      '
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: true
    name: incorrect_answers
    reference: ''
