dataset: amazon_counterfactual_en
templates:
  6eb62aee-a983-4571-9d49-9836e685ee93: !Template
    answer_choices: Yes ||| No
    id:  6eb62aee-a983-4571-9d49-9836e685ee93
    jinja: "{{text}} Is the statement factual? \n|||\n
      {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - en
      metrics:
      - Accuracy
      original_task: true
    name: is_factual
    reference: ''
  6fc17a35-e1a3-4b5d-87d5-91d0fdf42d58: !Template
    answer_choices: Yes ||| No
    id:  6fc17a35-e1a3-4b5d-87d5-91d0fdf42d58
    jinja: "{{text}} Does the statement describe a fact? \n|||\n
      {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - en
      metrics:
      - Accuracy
      original_task: true
    name: describe_fact
    reference: ''
  e36e9d87-5366-4070-b95d-51ff5a890f4b: !Template
    answer_choices: non-counterfactual ||| counterfactual
    id:  e36e9d87-5366-4070-b95d-51ff5a890f4b
    jinja: "{{text}} Is the statement non-counterfactual or counterfactual? \n|||\n
      {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      languages:
      - en
      metrics:
      - Accuracy
      original_task: true
    name: choice_text_before
    reference: ''
  044ee01d-a06d-47b6-9872-6c89f785a961: !Template
    answer_choices: No ||| Yes
    id:  044ee01d-a06d-47b6-9872-6c89f785a961
    jinja: "{{text}} Is the statement counterfactual? \n|||\n
      {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - en
      metrics:
      - Accuracy
      original_task: true
    name: is_counterfactual
    reference: ''
  20c40ee7-ba1e-4e65-bb55-1c7b4e4cbcf7: !Template
    answer_choices: No ||| Yes
    id:  20c40ee7-ba1e-4e65-bb55-1c7b4e4cbcf7
    jinja: "{{text}} Does the sentence express an event that did not happen? \n|||\n
      {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - en
      metrics:
      - Accuracy
      original_task: true
    name: did_not_happen
    reference: ''
  920ace8e-e063-4edf-b4f6-ac1a1e9f8559: !Template
    answer_choices: Yes ||| No
    id:  920ace8e-e063-4edf-b4f6-ac1a1e9f8559
    jinja: "{{text}} Does this describe an actual event? \n|||\n
      {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - en
      metrics:
      - Accuracy
      original_task: true
    name: actual_event
    reference: ''
  9e2a56f5-a11d-497c-b02b-2ccc3e760503: !Template
    answer_choices: Yes ||| No
    id:  9e2a56f5-a11d-497c-b02b-2ccc3e760503
    jinja: "{{text}} Does the sentence contain events that did not or cannot take place? \n|||\n
      {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - en
      metrics:
      - Accuracy
      original_task: true
    name: take_place
    reference: ''
  40c0007f-78be-43d0-80c8-df22d37ee64b: !Template
    answer_choices: non-counterfactual ||| counterfactual
    id:  40c0007f-78be-43d0-80c8-df22d37ee64b
    jinja: "Is the label for the following sentence non-counterfactual or counterfactual? {{text}} \n|||\n
      {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      languages:
      - en
      metrics:
      - Accuracy
      original_task: true
    name: choice_text_after
    reference: ''
