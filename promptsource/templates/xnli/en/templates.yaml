dataset: xnli
subset: en
templates:
  4e122d26-7e79-49c0-961b-cf8ee134759e: !Template
    answer_choices: No ||| Neutral ||| Yes
    id: 4e122d26-7e79-49c0-961b-cf8ee134759e
    jinja: 'Sentence 1: {{premise}}

      Sentence 2: {{hypothesis}}

      Question: Does Sentence 1 contradict Sentence 2? Yes, No, or {{"Neutral"}}?
      |||

      {{answer_choices[label]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: Concatenation contraposition
    reference: Concatenation contraposition
  a1506390-8d7a-4b8f-922c-6135e23094d7: !Template
    answer_choices: must be true ||| might be true ||| must be false
    id: a1506390-8d7a-4b8f-922c-6135e23094d7
    jinja: Given that {{premise}}, it {{"must be true, might be true, or must be false"}}
      that {{hypothesis}}? ||| It {{ answer_choices[label] }}.
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: true
    name: "given\u2026 must be true that\u2026"
    reference: 'Maybe a little verbose for a generative model, but anecdotally this
      is the most natural way of how I say an NLI sentence pair out loud to humans.
      Caveat: NLI annotations are not meant to be strictly truth-conditional entailment,
      so "must" is not ideal.'
  c62a3048-018e-4d93-bc46-645f3f763ee6: !Template
    answer_choices: No ||| No ||| Yes
    id: c62a3048-018e-4d93-bc46-645f3f763ee6
    jinja: 'Sentence 1: {{premise}}

      Sentence 2: {{hypothesis}}

      Question: Does Sentence 1 contradict Sentence 2? Yes or No? |||

      {{answer_choices[label]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: Label binarization contraposition
    reference: Inspired from https://arxiv.org/pdf/1902.01007.pdf Section 4 - Implementation
      and evaluation
  d027ab50-a86b-45ba-99fa-018c0e4cac4a: !Template
    answer_choices: Yes ||| Maybe ||| No
    id: d027ab50-a86b-45ba-99fa-018c0e4cac4a
    jinja: Given that {{premise}} Does it follow that {{hypothesis}} Yes, no, or maybe?
      ||| {{ answer_choices[label] }}
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: true
    name: "given\u2026 does it follow that\u2026 "
    reference: "\"Does it follow that\" could be replaced with \"can we infer that\u2026\
      \ \", \"is it guaranteed that\u2026\", etc. Ideally there should be a question\
      \ mark after \"does it follow that {hypothesis}?\", but the hypothesis string\
      \ often comes with ending punctuations of its own."
  d9e13133-267e-46c4-afad-c2379dcc5272: !Template
    answer_choices: True ||| Neither ||| False
    id: d9e13133-267e-46c4-afad-c2379dcc5272
    jinja: "{{premise}}\nQuestion: {{hypothesis}} True, False, or Neither? ||| \n\
      {{answer_choices[label]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: true
    name: ANLI GPT3
    reference: ANLI prompt format from Table G7 in the GPT3 paper
  dd4276e6-aebd-44a3-b3cf-baf8a4c237f0: !Template
    answer_choices: Yes ||| No ||| No
    id: dd4276e6-aebd-44a3-b3cf-baf8a4c237f0
    jinja: 'Sentence 1: {{premise}}

      Sentence 2: {{hypothesis}}

      Question: Does Sentence 1 entail Sentence 2? Yes or No? |||

      {{answer_choices[label]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: false
    name: Label binarization
    reference: Grouping "neutral" and "contradiction" as a single label following
      https://arxiv.org/pdf/1902.01007.pdf Section 4 - Implementation and evaluation
  ddfd2eb1-96a4-42db-ad5e-91d7cb011b4e: !Template
    answer_choices: Yes ||| Maybe ||| No
    id: ddfd2eb1-96a4-42db-ad5e-91d7cb011b4e
    jinja: '{{premise}} Based on the previous passage, is it true that {{hypothesis}}
      Yes, no, or maybe? ||| {{ answer_choices[label] }}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: true
    name: based on the previous passage
    reference: "Adapted from the BoolQ prompts in Schick & Sch\xFCtze 2021."
  e174f56a-b0af-4937-b6ae-1897cac26eba: !Template
    answer_choices: Yes ||| Neutral ||| No
    id: e174f56a-b0af-4937-b6ae-1897cac26eba
    jinja: 'Sentence 1: {{premise}}

      Sentence 2: {{hypothesis}}

      Question: Does Sentence 1 entail Sentence 2? Yes, No, or {{"Neutral"}}? |||

      {{answer_choices[label]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: null
      metrics: []
      original_task: true
    name: Concatenation
    reference: Concatenation of premise and hypothesis
