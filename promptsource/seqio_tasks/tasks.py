import functools

import datasets
import seqio
import t5
import tensorflow as tf

import promptsource.templates

from . import utils


# Datasets that don't work currently...
DATASET_BLACKLIST = [
    ("species_800", None),
    ("drop", None),
    ("discofuse", "discofuse-sport"),
    ("discofuse", "discofuse-wikipedia"),
    ("adversarial_qa", "adversarialQA"),
    ("tweet_eval", "emotion"),
    ("tweet_eval", "emoji"),
    ("tweet_eval", "hate"),
    ("tweet_eval", "offensive"),
    ("tweet_eval", "stance_atheism"),
    ("tweet_eval", "stance_abortion"),
    ("tweet_eval", "stance_feminist"),
    ("tweet_eval", "stance_climate"),
    ("tweet_eval", "sentiment"),
    ("tweet_eval", "stance_hillary"),
    ("tweet_eval", "irony"),
]

all_templates = promptsource.templates.TemplateCollection()

for dataset_name, subset_name in all_templates.keys:

    if (dataset_name, subset_name) in DATASET_BLACKLIST:
        continue

    dataset_splits = utils.get_dataset_splits(dataset_name, subset_name)
    templates = all_templates.get_dataset(dataset_name, subset_name)

    for template_name in templates.all_template_names:

        template = templates[template_name]

        def dataset_fn(split, shuffle_files, seed, dataset_name, subset_name, template):
            # HF datasets does not support file-level shuffling
            del shuffle_files, seed
            dataset = datasets.load_dataset(dataset_name, subset_name)
            dataset = dataset[split]
            dataset = utils.apply_template(dataset, template)
            return utils.hf_dataset_to_tf_dataset(dataset)

        seqio.TaskRegistry.add(
            utils.get_task_name(dataset_name, subset_name, template_name),
            seqio.FunctionDataSource(
                functools.partial(
                    dataset_fn,
                    seed=None,
                    dataset_name=dataset_name,
                    subset_name=subset_name,
                    template=template,
                ),
                splits=list(dataset_splits.keys()),
                num_input_examples={s: dataset_splits[s].num_examples for s in dataset_splits.keys()},
            ),
            preprocessors=[
                seqio.preprocessors.tokenize,
                seqio.preprocessors.append_eos,
                seqio.CacheDatasetPlaceholder(required=False),
            ],
            output_features={
                "inputs": seqio.Feature(t5.data.get_default_vocabulary(), add_eos=False, dtype=tf.int32),
                "targets": seqio.Feature(t5.data.get_default_vocabulary(), add_eos=True, dtype=tf.int32),
            },
            metric_fns=[t5.evaluation.metrics.sequence_accuracy],
        )

TASK_BLACKLIST = [
    # Tasks which often tokenize to > 1024 tokens currently
    "hotpot_qa_distractor_Generate_Explanations",
    "hotpot_qa_fullwiki_Generate_Explanations",
    "hotpot_qa_distractor_Generate_Answer_and_Explanations",
    "hotpot_qa_fullwiki_Generate_Answer_and_Explanations",
    "hotpot_qa_fullwiki_Generate_Answer",
    "hotpot_qa_distractor_Generate_Answer",
    "hotpot_qa_distractor_Generate_Title_2",
    "hotpot_qa_fullwiki_Generate_Title_2",
    "hotpot_qa_fullwiki_Generate_Title_1",
    "hotpot_qa_distractor_Generate_Title_1",
    "hotpot_qa_distractor_Generate_Question",
    "hotpot_qa_fullwiki_Generate_Question",
    "tab_fact_tab_fact_tab_fact_3",
    "tab_fact_tab_fact_tab_fact_2",
    "tab_fact_tab_fact_tab_fact_1",
    "tab_fact_tab_fact_tab_fact_7",
    "tab_fact_tab_fact_tab_fact_4",
    "tab_fact_tab_fact_tab_fact_5",
    "tab_fact_tab_fact_tab_fact_6",
    "wiki_hop_masked_Choose_Best_Object_Candidate",
    "wiki_hop_masked_Indirect_Question_about_Birthplace_Citizenship_Place_of_Death",
    "narrativeqa_Template_05",
    "ecthr_cases_alleged_violation_prediction_silver_rationales",
    # Tasks with broken cached files
    "gigaword_summarize_",
]

seqio.MixtureRegistry.add(
    "all_tasks_combined_max_1m",
    [task for task in seqio.TaskRegistry.names() if task not in TASK_BLACKLIST],
    default_rate=functools.partial(seqio.mixing_rate_num_examples, maximum=1000000),
)

seqio.MixtureRegistry.add(
    "all_super_glue_tasks",
    [task for task in seqio.TaskRegistry.names() if task.startswith("super_glue")],
    default_rate=seqio.mixing_rate_num_examples,
)

# Tasks deemed as clean/useful
CLEAN_TASKS = [
    "adversarial_qa_dbert_adversarial_qa_dbert_1",
    "adversarial_qa_dbert_adversarial_qa_dbert_10",
    "adversarial_qa_dbert_adversarial_qa_dbert_2",
    "adversarial_qa_dbert_adversarial_qa_dbert_3",
    "adversarial_qa_dbert_adversarial_qa_dbert_4",
    "adversarial_qa_dbert_adversarial_qa_dbert_5",
    "adversarial_qa_dbert_adversarial_qa_dbert_6",
    "adversarial_qa_dbert_adversarial_qa_dbert_7",
    "adversarial_qa_dbert_adversarial_qa_dbert_8",
    "adversarial_qa_dbert_adversarial_qa_dbert_9",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_1",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_10",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_2",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_3",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_4",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_5",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_6",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_7",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_8",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_9",
    "adversarial_qa_droberta_adversarial_qa_droberta_1",
    "adversarial_qa_droberta_adversarial_qa_droberta_10",
    "adversarial_qa_droberta_adversarial_qa_droberta_2",
    "adversarial_qa_droberta_adversarial_qa_droberta_3",
    "adversarial_qa_droberta_adversarial_qa_droberta_4",
    "adversarial_qa_droberta_adversarial_qa_droberta_5",
    "adversarial_qa_droberta_adversarial_qa_droberta_6",
    "adversarial_qa_droberta_adversarial_qa_droberta_7",
    "adversarial_qa_droberta_adversarial_qa_droberta_8",
    "adversarial_qa_droberta_adversarial_qa_droberta_9",
    "aeslc_context_question_1",
    "aeslc_context_question_2",
    "aeslc_context_question_3",
    "aeslc_context_question_4",
    "aeslc_question_context_1",
    "aeslc_question_context_2",
    "aeslc_question_context_3",
    "aeslc_question_context_4",
    "ag_news_classify",
    "ag_news_classify_with_choices",
    "ag_news_recommend",
    "ag_news_which_section",
    "ag_news_which_section_choices",
    "amazon_polarity_Template_1",
    "amazon_polarity_Template_2",
    "amazon_polarity_Template_3",
    "amazon_polarity_Template_4",
    "amazon_polarity_Template_5",
    "amazon_polarity_Template_6",
    "app_reviews_categorize_rating_using_review",
    "app_reviews_convert_to_rating",
    "app_reviews_convert_to_star_rating",
    "app_reviews_generate_review",
    "ai2_arc_ARC_Challenge_answer_qn",
    "ai2_arc_ARC_Challenge_false",
    "ai2_arc_ARC_Challenge_qa_options",
    "ai2_arc_ARC_Challenge_test",
    "ai2_arc_ARC_Easy_answer_qn",
    "ai2_arc_ARC_Easy_false",
    "ai2_arc_ARC_Easy_qa_options",
    "ai2_arc_ARC_Easy_test",
    "circa_goldstandard1_judgement",
    "circa_goldstandard2_judgement",
    "circa_judgement",
    "circa_possible_qn",
    "circa_question_declarative",
    "climate_fever_claim_and_all_supporting_evidences",
    "climate_fever_fifth_evidence_and_claim_itemization",
    "climate_fever_fifth_evidence_claim_pair",
    "climate_fever_first_evidence_and_claim_itemization",
    "climate_fever_first_evidence_claim_pair",
    "climate_fever_fourth_evidence_and_claim_itemization",
    "climate_fever_fourth_evidence_claim_pair",
    "climate_fever_second_evidence_and_claim_itemization",
    "climate_fever_second_evidence_claim_pair",
    "climate_fever_third_evidence_and_claim_itemization",
    "climate_fever_third_evidence_claim_pair",
    "codah_codah_answer_no_option",
    "codah_codah_answer_with_option",
    "codah_codah_answer_with_option_idx",
    "codah_codah_answer_with_option_post",
    "codah_codah_choose_from_list",
    "codah_codah_finish_from_the_list",
    "codah_codah_finish_from_the_list_post",
    "codah_codah_finish_pre",
    "codah_codah_question_category",
    "codah_codah_question_category_bis",
    "common_gen_Example_prompt",
    "common_gen_Given_concepts",
    "common_gen_Put_together",
    "common_gen_choice_in_concept_centric_sentence_generation",
    "common_gen_sentence_to_concepts",
    "commonsense_qa_answer_qn",
    "commonsense_qa_most_suitable",
    "commonsense_qa_not_multiple_choice",
    "commonsense_qa_qa",
    "commonsense_qa_test_question",
    "commonsense_qa_test_question_false",
    "cos_e_v1.11_description_question_option_id",
    "cos_e_v1.11_description_question_option_text",
    "cos_e_v1.11_generate_explanation_given_text",
    "cos_e_v1.11_generate_explanation_no_given_answer",
    "cos_e_v1.11_question_description_option_id",
    "cos_e_v1.11_question_description_option_text",
    "cos_e_v1.11_question_option_description_id",
    "cos_e_v1.11_question_option_description_text",
    "cosmos_qa_context_description_question_answer_id",
    "cosmos_qa_context_description_question_answer_text",
    "cosmos_qa_context_description_question_text",
    "cosmos_qa_context_question_answer_description_id",
    "cosmos_qa_context_question_answer_description_text",
    "cosmos_qa_context_question_description_answer_id",
    "cosmos_qa_context_question_description_answer_text",
    "cosmos_qa_context_question_description_text",
    "cosmos_qa_description_context_question_answer_id",
    "cosmos_qa_description_context_question_answer_text",
    "cosmos_qa_description_context_question_text",
    "cosmos_qa_no_prompt_id",
    "cosmos_qa_no_prompt_text",
    "dbpedia_14_dbpedia_1",
    "dbpedia_14_dbpedia_10",
    "dbpedia_14_dbpedia_3",
    "dbpedia_14_dbpedia_5",
    "dbpedia_14_dbpedia_7",
    "dbpedia_14_dbpedia_8",
    "dbpedia_14_dbpedia_9",
    "discovery_discovery_connector",
    "discovery_discovery_correction",
    "discovery_discovery_discourse",
    "discovery_discovery_make_sense",
    "discovery_discovery_transition",
    "dream_answer_to_dialogue",
    "dream_baseline",
    "dream_conversation",
    "dream_generate_first_utterance",
    "dream_generate_last_utterance",
    "duorc_SelfRC_build_story_around_qa",
    "duorc_SelfRC_generate_question",
    "duorc_SelfRC_generate_question_by_answer",
    "duorc_SelfRC_question_answering",
    "duorc_SelfRC_title_generation",
    "duorc_ParaphraseRC_answerable_question",
    "duorc_ParaphraseRC_build_story_around_qa",
    "duorc_ParaphraseRC_generate_question",
    "duorc_ParaphraseRC_generate_question_by_answer",
    "duorc_ParaphraseRC_question_answering",
    "duorc_ParaphraseRC_title_generation",
    "emo_feeling",
    "emo_final_message",
    "emo_persons_describe",
    "emo_persons_infer",
    "emo_spoke_last",
    "emotion_Template_0",
    "emotion_Template_1",
    "emotion_Template_2",
    "emotion_Template_3",
    "emotion_Template_4",
    "emotion_Template_5",
    "freebase_qa_inference_chain_prompt",
    "freebase_qa_inference_chain_prompt_context",
    "freebase_qa_qa_context_1",
    "freebase_qa_qa_context_2",
    "freebase_qa_qa_template_basic",
    "gigaword_Document_",
    "gigaword_Summarize_this_document_",
    "gigaword_TLDR",
    "gigaword_generate_summary_for_this",
    "gigaword_in_a_nutshell",
    "gigaword_reverse_writing",
    "gigaword_reverse_writing_2",
    "gigaword_summarize_",
    "gigaword_write_one_sentence",
    "glue_cola_Following_sentence_acceptable",
    "glue_cola_Make_sense_yes_no",
    "glue_cola_Previous_sentence_acceptable",
    "glue_cola_editing",
    "glue_cola_jinja_example",
    "glue_mrpc_equivalent",
    "glue_mrpc_paraphrase",
    "glue_mrpc_replace",
    "glue_mrpc_same_thing",
    "glue_mrpc_want_to_know",
    "glue_qqp_answer",
    "glue_qqp_duplicate",
    "glue_qqp_duplicate_or_not",
    "glue_qqp_quora",
    "glue_qqp_same_thing",
    "glue_sst2_following_positive_negative",
    "glue_sst2_happy_or_mad",
    "glue_sst2_positive_negative_after",
    "glue_sst2_review",
    "glue_sst2_said",
    "glue_stsb_examples",
    "glue_stsb_rank",
    "glue_stsb_rate",
    "glue_stsb_score",
    "glue_stsb_similarity",
    "hellaswag_YesNo_0",
    "hellaswag_YesNo_1",
    "hellaswag_YesNo_2",
    "hellaswag_YesNo_3",
    "hellaswag_YesNo_reversed_0",
    "hellaswag_YesNo_reversed_1",
    "hellaswag_YesNo_reversed_2",
    "hellaswag_YesNo_reversed_3",
    "hellaswag_complete_first_then",
    "hellaswag_first_then",
    "hellaswag_how_ends",
    "hellaswag_if_begins_how_continues",
    "hellaswag_which_ending",
    "imdb_imdb_1",
    "imdb_imdb_2",
    "imdb_imdb_3",
    "imdb_imdb_4",
    "imdb_imdb_5",
    "imdb_imdb_6",
    "imdb_imdb_7",
    "imdb_imdb_8",
    "imdb_imdb_9",
    "fever_v2.0_cbqa_fever_dialog_style_postprompt",
    "fever_v2.0_cbqa_fever_dialog_style_surrounded",
    "fever_v2.0_cbqa_fever_postprompt",
    "fever_v2.0_cbqa_fever_preprompt",
    "fever_v2.0_cbqa_fever_short",
    "hotpot_qa_distractor_Generate_Answer",
    "hotpot_qa_distractor_Generate_Answer_and_Explanations",
    "hotpot_qa_distractor_Generate_Explanations",
    "hotpot_qa_distractor_Generate_Question",
    "hotpot_qa_distractor_Generate_Title_1",
    "hotpot_qa_distractor_Generate_Title_2",
    "hotpot_qa_fullwiki_Classify_Question_Type",
    "hotpot_qa_fullwiki_Generate_Answer",
    "hotpot_qa_fullwiki_Generate_Answer_and_Explanations",
    "hotpot_qa_fullwiki_Generate_Explanations",
    "hotpot_qa_fullwiki_Generate_Question",
    "hotpot_qa_fullwiki_Generate_Title_1",
    "hotpot_qa_fullwiki_Generate_Title_2",
    "limit_any_entity",
    "limit_any_entity_indirect",
    "limit_count_entities",
    "limit_count_entities_affirm",
    "limit_find_entities_affirm",
    "limit_find_entities_extract",
    "limit_find_entities_question",
    "limit_first_moving_entity",
    "limit_last_moving_entity",
    "limit_more_than_one",
    "mc_taco_mc_taco_1",
    "mc_taco_mc_taco_2",
    "mc_taco_mc_taco_3",
    "mc_taco_mc_taco_4",
    "mc_taco_mc_taco_5",
    "mc_taco_mc_taco_6",
    "narrativeqa_Template_01",
    "narrativeqa_Template_02",
    "narrativeqa_Template_03",
    "narrativeqa_Template_04",
    "narrativeqa_Template_05",
    "narrativeqa_Template_06",
    "narrativeqa_Template_07",
    "narrativeqa_Template_08",
    "numer_sense_fill_in_the_blank_v1",
    "numer_sense_fill_in_the_blank_v2",
    "numer_sense_fill_in_the_blank_v3",
    "numer_sense_fill_in_the_blank_v5",
    "numer_sense_fill_in_the_blank_v6",
    "numer_sense_fill_in_the_blank_v8",
    "onestop_english_ara_context",
    "onestop_english_assess",
    "onestop_english_ats",
    "onestop_english_esl_context",
    "onestop_english_esl_variation",
    "openbookqa_main_choices",
    "openbookqa_main_choose_an_answer_with_options",
    "openbookqa_main_only_options",
    "openbookqa_main_pick_answer_with_options",
    "openbookqa_main_pick_using_id",
    "openbookqa_main_which_correct",
    "openbookqa_main_which_correct_inverse",
    "paws_labeled_final_Concatenation",
    "paws_labeled_final_Concatenation_no_label",
    "paws_labeled_final_Meaning",
    "paws_labeled_final_Meaning_no_label",
    "paws_labeled_final_PAWS_ANLI_GPT3",
    "paws_labeled_final_PAWS_ANLI_GPT3_no_label",
    "paws_labeled_final_Rewrite",
    "paws_labeled_final_Rewrite_no_label",
    "paws_labeled_final_context_question",
    "paws_labeled_final_context_question_no_label",
    "paws_labeled_final_paraphrase_task",
    "paws_labeled_final_paraphrase_task_reverse",
    "paws_labeled_final_task_description_no_label",
    "paws_labeled_swap_Concatenation",
    "paws_labeled_swap_Concatenation_no_label",
    "paws_labeled_swap_Meaning",
    "paws_labeled_swap_Meaning_no_label",
    "paws_labeled_swap_PAWS_ANLI_GPT3",
    "paws_labeled_swap_PAWS_ANLI_GPT3_no_label",
    "paws_labeled_swap_Rewrite",
    "paws_labeled_swap_Rewrite_no_label",
    "paws_labeled_swap_context_question",
    "paws_labeled_swap_context_question_no_label",
    "paws_labeled_swap_paraphrase_task",
    "paws_labeled_swap_paraphrase_task_reverse",
    "paws_labeled_swap_task_description_no_label",
    "paws_unlabeled_final_Concatenation",
    "paws_unlabeled_final_Concatenation_no_label",
    "paws_unlabeled_final_Meaning",
    "paws_unlabeled_final_Meaning_no_label",
    "paws_unlabeled_final_PAWS_ANLI_GPT3",
    "paws_unlabeled_final_PAWS_ANLI_GPT3_no_label",
    "paws_unlabeled_final_Rewrite",
    "paws_unlabeled_final_Rewrite_no_label",
    "paws_unlabeled_final_context_question",
    "paws_unlabeled_final_context_question_no_label",
    "paws_unlabeled_final_paraphrase_task",
    "paws_unlabeled_final_paraphrase_task_reverse",
    "paws_unlabeled_final_task_description_no_label",
    "paws_x_en_Concatenation",
    "paws_x_en_Concatenation_no_label",
    "paws_x_en_Meaning",
    "paws_x_en_Meaning_no_label",
    "paws_x_en_PAWS_ANLI_GPT3",
    "paws_x_en_PAWS_ANLI_GPT3_no_label",
    "paws_x_en_Rewrite",
    "paws_x_en_Rewrite_no_label",
    "paws_x_en_context_question",
    "paws_x_en_context_question_no_label",
    "paws_x_en_paraphrase_task",
    "paws_x_en_paraphrase_task_reverse",
    "paws_x_en_task_description_no_label",
    "piqa_Correct_the_solution",
    "piqa_Correct_the_solution_if_false_from_sol_1",
    "piqa_Correct_the_solution_if_false_from_sol_2",
    "piqa_Does_this_solution_make_sense_sol1",
    "piqa_Does_this_solution_make_sense_sol2",
    "piqa_Generate_a_similar_but_wrong_solution",
    "piqa_choose_the_most_appropriate_solution",
    "piqa_choose_the_most_appropriate_solution_reorder_solution",
    "piqa_no_prompt_needed",
    "qa_srl_aq",
    "qa_srl_context_answer",
    "qa_srl_context_qn",
    "qa_srl_predicate",
    "qa_srl_qa",
    "qasc_is_correct_0",
    "qasc_is_correct_1",
    "qasc_qu_combined",
    "qasc_sep_combined_can_tell",
    "qasc_sep_qu",
    "quail_context_description_question_answer_id",
    "quail_context_description_question_answer_text",
    "quail_context_description_question_text",
    "quail_context_question_answer_description_id",
    "quail_context_question_answer_description_text",
    "quail_context_question_description_answer_id",
    "quail_context_question_description_answer_text",
    "quail_context_question_description_text",
    "quail_description_context_question_answer_id",
    "quail_description_context_question_answer_text",
    "quail_description_context_question_text",
    "quail_no_prompt_id",
    "quail_no_prompt_text",
    "quartz_para_question_1",
    "quartz_para_question_1_reverse",
    "quartz_para_question_2",
    "quartz_para_question_3_choices",
    "quartz_para_question_4_choices",
    "quartz_para_question_plain",
    "quartz_para_question_plain_reverse",
    "quartz_question_para_1",
    "quartz_question_para_1_reverse",
    "quartz_question_para_2",
    "quartz_question_para_3",
    "quartz_question_para_3_reverse",
    "quoref_Template_1",
    "quoref_Template_2",
    "quoref_Template_3",
    "quoref_Template_4",
    "quoref_Template_5",
    "race_high_Read_the_article_and_answer_the_question_no_option_",
    "race_high_Read_the_article_and_select_the_best_answer",
    "race_high_Read_the_article_and_select_the_best_answer2",
    "race_high_Read_the_article_and_select_the_best_answer3",
    "race_high_Write_a_multi_choice_question_for_the_following_article",
    "race_high_Write_a_multi_choice_question_for_the_following_article_2",
    "race_middle_Read_the_article_and_answer_the_question_no_option_",
    "race_middle_Read_the_article_and_select_the_best_answer",
    "race_middle_Read_the_article_and_select_the_best_answer2",
    "race_middle_Read_the_article_and_select_the_best_answer3",
    "race_middle_Write_a_multi_choice_question_for_the_following_article",
    "race_middle_Write_a_multi_choice_question_for_the_following_article_2",
    "ropes_funky_prompt",
    "ropes_plain",
    "ropes_plain_bottom_hint",
    "ropes_plain_no_background",
    "ropes_prompt_beginning",
    "ropes_prompt_bottom_hint_beginning",
    "ropes_prompt_bottom_no_hint",
    "ropes_prompt_mix",
    "rotten_tomatoes_rt_1",
    "rotten_tomatoes_rt_10",
    "rotten_tomatoes_rt_2",
    "rotten_tomatoes_rt_3",
    "rotten_tomatoes_rt_4",
    "rotten_tomatoes_rt_5",
    "rotten_tomatoes_rt_6",
    "rotten_tomatoes_rt_7",
    "rotten_tomatoes_rt_8",
    "rotten_tomatoes_rt_9",
    "sciq_Template_0",
    "sciq_Template_1",
    "social_i_qa_social_i_qa1",
    "social_i_qa_social_i_qa2",
    "social_i_qa_social_i_qa3",
    "social_i_qa_social_i_qa4",
    "social_i_qa_social_i_qa5",
    "squad_adversarial_AddSent_after",
    "squad_adversarial_AddSent_answers_question",
    "squad_adversarial_AddSent_cbqa",
    "squad_adversarial_AddSent_cbqa_qa",
    "squad_adversarial_AddSent_cbqa_question_answer",
    "squad_adversarial_AddSent_count_letters",
    "squad_adversarial_AddSent_exam",
    "squad_adversarial_AddSent_exam_creation_help",
    "squad_adversarial_AddSent_find_text",
    "squad_adversarial_AddSent_generate_question",
    "squad_adversarial_AddSent_incorrect_answers",
    "squad_adversarial_AddSent_possible_pitfalls",
    "squad_adversarial_AddSent_possible_qn",
    "squad_adversarial_AddSent_question_hint",
    "squad_adversarial_AddSent_title",
    "squad_adversarial_AddSent_wondered",
    "squad_after",
    "squad_cbqa",
    "squad_cbqa_qa",
    "squad_cbqa_question_answer",
    "squad_count_letters",
    "squad_exam",
    "squad_exam_creation_help",
    "squad_find",
    "squad_find_text",
    "squad_generate_question",
    "squad_question_hint",
    "squad_v2_Jeopardy_with_Context",
    "squad_v2_Jeopardy_without_Context",
    "squad_v2_Questions_with_Context",
    "squad_v2_Questions_with_Context_Without_Prompt_Keywords",
    "squad_v2_Topic_Prediction_Context",
    "squad_v2_Topic_Prediction_Context_with_randomized_prompt_options",
    "squad_v2_Topic_Prediction_Context_with_randomized_prompt_options_placed_in_the_end",
    "squad_v2_Topic_Prediction_Question_and_Answer_Pair",
    "squad_v2_Trivia",
    "squad_wondered",
    "squadshifts_amazon_after",
    "squadshifts_amazon_answers_question",
    "squadshifts_amazon_cbqa",
    "squadshifts_amazon_cbqa_qa",
    "squadshifts_amazon_cbqa_question_answer",
    "squadshifts_amazon_count_letters",
    "squadshifts_amazon_exam",
    "squadshifts_amazon_exam_creation_help",
    "squadshifts_amazon_find_text",
    "squadshifts_amazon_generate_question",
    "squadshifts_amazon_incorrect_answers",
    "squadshifts_amazon_possible_pitfalls",
    "squadshifts_amazon_possible_qn",
    "squadshifts_amazon_question_hint",
    "squadshifts_amazon_title",
    "squadshifts_amazon_wondered",
    "squadshifts_new_wiki_after",
    "squadshifts_new_wiki_answers_question",
    "squadshifts_new_wiki_cbqa",
    "squadshifts_new_wiki_cbqa_qa",
    "squadshifts_new_wiki_cbqa_question_answer",
    "squadshifts_new_wiki_count_letters",
    "squadshifts_new_wiki_exam",
    "squadshifts_new_wiki_exam_creation_help",
    "squadshifts_new_wiki_find_text",
    "squadshifts_new_wiki_generate_question",
    "squadshifts_new_wiki_incorrect_answers",
    "squadshifts_new_wiki_possible_pitfalls",
    "squadshifts_new_wiki_possible_qn",
    "squadshifts_new_wiki_question_hint",
    "squadshifts_new_wiki_title",
    "squadshifts_new_wiki_wondered",
    "squadshifts_nyt_after",
    "squadshifts_nyt_answers_question",
    "squadshifts_nyt_cbqa",
    "squadshifts_nyt_cbqa_qa",
    "squadshifts_nyt_cbqa_question_answer",
    "squadshifts_nyt_count_letters",
    "squadshifts_nyt_exam",
    "squadshifts_nyt_exam_creation_help",
    "squadshifts_nyt_find_text",
    "squadshifts_nyt_generate_question",
    "squadshifts_nyt_incorrect_answers",
    "squadshifts_nyt_possible_pitfalls",
    "squadshifts_nyt_possible_qn",
    "squadshifts_nyt_question_hint",
    "squadshifts_nyt_title",
    "squadshifts_nyt_wondered",
    "squad_v2_Jeopardy_with_Context",
    "squad_v2_Jeopardy_without_Context",
    "squad_v2_Questions_with_Context",
    "squad_v2_Questions_with_Context_Without_Prompt_Keywords",
    "squad_v2_Topic_Prediction_Context",
    "squad_v2_Topic_Prediction_Context_with_randomized_prompt_options",
    "squad_v2_Topic_Prediction_Context_with_randomized_prompt_options_placed_in_the_end",
    "squad_v2_Topic_Prediction_Question_and_Answer_Pair",
    "squad_v2_Trivia",
    "super_glue_boolq_GPT_3_Style",
    "super_glue_boolq_I_wonder_",
    "super_glue_boolq_based_on_the_following_passage",
    "super_glue_boolq_based_on_the_previous_passage",
    "super_glue_boolq_could_you_tell_me_",
    "super_glue_cb_GPT_3_style",
    "super_glue_cb_based_on_the_previous_passage",
    "super_glue_cb_does_S1_contradict_S2_",
    "super_glue_cb_does_S1_entail_S2_",
    "super_glue_cb_given_does_it_follow_that_",
    "super_glue_cb_given_it_must_be_true_that_",
    "super_glue_copa_C1_or_C2_premise_so_because_",
    "super_glue_copa__As_a_result_C1_or_C2_",
    "super_glue_copa__What_could_happen_next_C1_or_C2_",
    "super_glue_copa__which_may_be_caused_by",
    "super_glue_copa__which_may_cause_C1_or_C2_",
    "super_glue_copa__why_C1_or_C2",
    "super_glue_multirc_I_was_going_to_say_",
    "super_glue_multirc_Would_it_be_good_to_answer_",
    "super_glue_multirc_is_a_correct_answer_",
    "super_glue_multirc_is_the_correct_answer_",
    "super_glue_multirc_paragraph_question_is_it_",
    "super_glue_record_Can_you_figure_out_",
    "super_glue_record_In_the_question_above_the_placeholder_stands_for",
    "super_glue_record_What_could_the_placeholder_be_",
    "super_glue_record_Which_one_is_the_placeholder_",
    "super_glue_record_the_placeholder_refers_to_",
    "super_glue_wic_GPT_3_prompt",
    "super_glue_wic_GPT_3_prompt_with_label",
    "super_glue_wic_question_context",
    "super_glue_wic_question_context_meaning",
    "super_glue_wic_question_context_meaning_with_label",
    "super_glue_wic_similar_sense",
    "super_glue_wsc.fixed_Here_p_stands_for_",
    "super_glue_wsc.fixed_In_the_previous_sentence_the_pronoun_refers_to_",
    "super_glue_wsc.fixed_Who_is_are_",
    "super_glue_wsc.fixed_in_the_passage_above_the_pronoun_X_refers_to_",
    "super_glue_wsc.fixed_passage_what_does_the_pronoun_refer_to_",
    "swag_regular_YesNo_0",
    "swag_regular_YesNo_1",
    "swag_regular_YesNo_2",
    "swag_regular_YesNo_3",
    "swag_regular_YesNo_reversed_0",
    "swag_regular_YesNo_reversed_1",
    "swag_regular_YesNo_reversed_2",
    "swag_regular_YesNo_reversed_3",
    "swag_regular_complete_first_then",
    "swag_regular_first_then",
    "swag_regular_how_ends",
    "swag_regular_if_begins_how_continues",
    "swag_regular_which_ending",
    "trec_fine_grained_ABBR",
    "trec_fine_grained_ABBR_context_first",
    "trec_fine_grained_DESC",
    "trec_fine_grained_DESC_context_first",
    "trec_fine_grained_ENTY",
    "trec_fine_grained_ENTY_context_first",
    "trec_fine_grained_HUM",
    "trec_fine_grained_HUM_context_first",
    "trec_fine_grained_LOC",
    "trec_fine_grained_LOC_context_first",
    "trec_fine_grained_NUM",
    "trec_fine_grained_NUM_context_first",
    "trec_fine_grained_open",
    "trec_fine_grained_open_context_first",
    "trec_trec1",
    "trec_trec2",
    "web_questions_count_answers",
    "web_questions_credible_question",
    "web_questions_if_answers_what_question",
    "web_questions_potential_correct_answer",
    "web_questions_question_answer",
    "web_questions_suggest_question",
    "wiki_bio_comprehension",
    "wiki_bio_guess_person",
    "wiki_bio_key_content",
    "wiki_bio_what_content",
    "wiki_bio_who",
    "wiki_qa_Direct_Answer_to_Question",
    "wiki_qa_Generate_Question_from_Topic",
    "wiki_qa_Is_This_True_",
    "wiki_qa_Jeopardy_style",
    "wiki_qa_Topic_Prediction_Answer_Only",
    "wiki_qa_Topic_Prediction_Question_Only",
    "wiki_qa_Topic_Prediction_Question_and_Answer_Pair",
    "wiki_hop_original_Choose_Best_Object_Candidate",
    "wiki_hop_original_Explain_Relation",
    "wiki_hop_original_Generate_Fact_Triple",
    "wiki_hop_original_Generate_Object_Answer",
    "wiki_hop_original_Generate_Subject_Answer",
    "wiki_hop_original_Indirect_Question_about_Birthplace_Citizenship_Place_of_Death",
    "wiqa_effect_with_label_answer",
    "wiqa_effect_with_string_answer",
    "wiqa_impacting_the_process",
    "wiqa_question_type",
    "wiqa_remove_first_step",
    "wiqa_remove_first_step_bis",
    "wiqa_remove_last_step",
    "wiqa_remove_last_step_bis",
    "xsum_Document_",
    "xsum_Summarize_this_document_",
    "xsum_TLDR",
    "xsum_generate_summary_for_this",
    "xsum_summarize_",
    "xsum_write_one_sentence",
    "yahoo_answers_topics_answer_from_qn",
    "yahoo_answers_topics_qa",
    "yahoo_answers_topics_subject",
    "yahoo_answers_topics_title_class",
    "yahoo_answers_topics_topic",
    "yelp_review_full_based_on_that",
    "yelp_review_full_format_rating",
    "yelp_review_full_format_score",
    "yelp_review_full_format_star",
    "yelp_review_full_on_a_scale",
    "yelp_review_full_so_i_would",
    "yelp_review_full_this_place",
]

seqio.MixtureRegistry.add(
    "clean_tasks",
    [task for task in CLEAN_TASKS if task not in TASK_BLACKLIST],
    default_rate=functools.partial(seqio.mixing_rate_num_examples, maximum=1000000),
)

# Tasks to evaluate models trained on clean_tasks
CLEAN_EVAL_TASKS = evaluation_sets = [
    'anli_GPT_3_style',
    'anli_based_on_the_previous_passage',
    'anli_does_S1_contradict_S2_',
    'anli_does_S1_entail_S2_',
    'anli_given_does_it_follow_that_',
    'anli_given_it_must_be_true_that_',

    'circa_goldstandard1_judgement',
    'circa_goldstandard2_judgement',
    'circa_judgement',

    'hans_GPT_3_style',
    'hans_Suppose_Can_we_infer_that_',
    'hans_based_on_the_previous_passage',
    'hans_does_S1_entail_S2_',
    'hans_given_does_it_follow_that_',
    'hans__does_the_previous_passage_support_the_claim_that',

    'limit_any_entity',
    'limit_any_entity_indirect',  # revisit
    'limit_find_entities_affirm',
    'limit_find_entities_extract',
    'limit_find_entities_question',

    # no train split
    'mc_taco_mc_taco_1',
    'mc_taco_mc_taco_3',  # awkward phrasing
    'mc_taco_mc_taco_6',  # revisit

    'openbookqa_main_choices',
    'openbookqa_main_choose_an_answer_with_options',
    'openbookqa_main_only_options',
    'openbookqa_main_pick_answer_with_options',
    'openbookqa_main_pick_using_id',
    'openbookqa_main_which_correct',
    # 'openbookqa_main_which_correct_inverse',  # seems to be a bug here, tempalte not filled

    'qa_srl_qa',

    # note eval metrics differ based on whether options are give vs. just generation
    'race_high_Read_the_article_and_answer_the_question_no_option_',
    'race_high_Read_the_article_and_select_the_best_answer',
    'race_high_Read_the_article_and_select_the_best_answer2',
    'race_high_Read_the_article_and_select_the_best_answer3',
    'race_middle_Read_the_article_and_answer_the_question_no_option_',
    'race_middle_Read_the_article_and_select_the_best_answer',
    'race_middle_Read_the_article_and_select_the_best_answer2',
    'race_middle_Read_the_article_and_select_the_best_answer3',

    'social_i_qa_social_i_qa1',
    'social_i_qa_social_i_qa2',
    'social_i_qa_social_i_qa3',  # revisit use of ordinal word
    'social_i_qa_social_i_qa5',  # is x a valid answer

    'super_glue_boolq_GPT_3_Style',
    'super_glue_boolq_I_wonder_',
    'super_glue_boolq_based_on_the_following_passage',
    'super_glue_boolq_based_on_the_previous_passage',
    'super_glue_boolq_could_you_tell_me_',
    'super_glue_cb_GPT_3_style',
    'super_glue_cb_based_on_the_previous_passage',
    'super_glue_cb_does_S1_contradict_S2_',
    'super_glue_cb_does_S1_entail_S2_',
    'super_glue_cb_given_does_it_follow_that_',
    'super_glue_cb_given_it_must_be_true_that_',
    'super_glue_copa_C1_or_C2_premise_so_because_',
    'super_glue_copa__As_a_result_C1_or_C2_',
    'super_glue_copa__What_could_happen_next_C1_or_C2_',
    'super_glue_copa__which_may_be_caused_by',
    'super_glue_copa__which_may_cause_C1_or_C2_',
    'super_glue_copa__why_C1_or_C2',
    'super_glue_multirc_I_was_going_to_say_',
    'super_glue_multirc_Would_it_be_good_to_answer_',
    'super_glue_multirc_is_a_correct_answer_',
    'super_glue_multirc_is_the_correct_answer_',
    'super_glue_multirc_paragraph_question_is_it_',
    'super_glue_record_Can_you_figure_out_',
    'super_glue_record_In_the_question_above_the_placeholder_stands_for',
    'super_glue_record_What_could_the_placeholder_be_',
    'super_glue_record_Which_one_is_the_placeholder_',
    'super_glue_record_the_placeholder_refers_to_',
    'super_glue_wic_GPT_3_prompt',
    'super_glue_wic_GPT_3_prompt_with_label',
    'super_glue_wic_question_context',
    'super_glue_wic_question_context_meaning',
    'super_glue_wic_question_context_meaning_with_label',
    'super_glue_wic_similar_sense',  # revisit wording in this prompt
    'super_glue_wsc.fixed_Here_p_stands_for_',
    'super_glue_wsc.fixed_In_the_previous_sentence_the_pronoun_refers_to_',
    'super_glue_wsc.fixed_Who_is_are_',
    'super_glue_wsc.fixed_in_the_passage_above_the_pronoun_X_refers_to_',
    'super_glue_wsc.fixed_passage_what_does_the_pronoun_refer_to_',

    # ARC and Hotpot probably slow to evaluate
    'ai2_arc_ARC_Easy_qa_options',
    'ai2_arc_ARC_Easy_test',
    'ai2_arc_ARC_Easy_false',  # pick incorrect options; negation possibly too hard
    'ai2_arc_ARC_Challenge_qa_options',
    'ai2_arc_ARC_Challenge_test',
    'ai2_arc_ARC_Challenge_false',  # pick incorrect options; negation possibly too hard
]

seqio.MixtureRegistry.add(
    "clean_eval_tasks",
    [task for task in CLEAN_EVAL_TASKS if task not in TASK_BLACKLIST],
    default_rate=functools.partial(seqio.mixing_rate_num_examples, maximum=1000000),
)
