import csv
import functools

import datasets
import seqio
import t5
import tensorflow as tf

import promptsource.templates

from . import utils


# Datasets that don't work currently...
DATASET_BLACKLIST = [
    ("species_800", None),
    ("drop", None),
    ("discofuse", "discofuse-sport"),
    ("discofuse", "discofuse-wikipedia"),
    ("adversarial_qa", "adversarialQA"),
    ("tweet_eval", "emotion"),
    ("tweet_eval", "emoji"),
    ("tweet_eval", "hate"),
    ("tweet_eval", "offensive"),
    ("tweet_eval", "stance_atheism"),
    ("tweet_eval", "stance_abortion"),
    ("tweet_eval", "stance_feminist"),
    ("tweet_eval", "stance_climate"),
    ("tweet_eval", "sentiment"),
    ("tweet_eval", "stance_hillary"),
    ("tweet_eval", "irony"),
]

all_templates = promptsource.templates.TemplateCollection()

for dataset_name, subset_name in all_templates.keys:

    if (dataset_name, subset_name) in DATASET_BLACKLIST:
        continue

    dataset_splits = utils.get_dataset_splits(dataset_name, subset_name)
    templates = all_templates.get_dataset(dataset_name, subset_name)

    for template_name in templates.all_template_names:

        template = templates[template_name]

        def dataset_fn(split, shuffle_files, seed, dataset_name, subset_name, template):
            # HF datasets does not support file-level shuffling
            del shuffle_files, seed
            dataset = datasets.load_dataset(dataset_name, subset_name)
            dataset = dataset[split]
            dataset = utils.apply_template(dataset, template)
            return utils.hf_dataset_to_tf_dataset(dataset)

        seqio.TaskRegistry.add(
            utils.get_task_name(dataset_name, subset_name, template_name),
            seqio.FunctionDataSource(
                functools.partial(
                    dataset_fn,
                    seed=None,
                    dataset_name=dataset_name,
                    subset_name=subset_name,
                    template=template,
                ),
                splits=list(dataset_splits.keys()),
                num_input_examples={s: dataset_splits[s].num_examples for s in dataset_splits.keys()},
            ),
            preprocessors=[
                seqio.preprocessors.tokenize,
                seqio.preprocessors.append_eos,
                seqio.CacheDatasetPlaceholder(required=False),
            ],
            output_features={
                "inputs": seqio.Feature(t5.data.get_default_vocabulary(), add_eos=False, dtype=tf.int32),
                "targets": seqio.Feature(t5.data.get_default_vocabulary(), add_eos=True, dtype=tf.int32),
            },
            metric_fns=[t5.evaluation.metrics.sequence_accuracy],
        )

TASK_BLACKLIST = [
    # Tasks which often tokenize to > 1024 tokens currently
    "hotpot_qa_distractor_Generate_Explanations",
    "hotpot_qa_fullwiki_Generate_Explanations",
    "hotpot_qa_distractor_Generate_Answer_and_Explanations",
    "hotpot_qa_fullwiki_Generate_Answer_and_Explanations",
    "hotpot_qa_fullwiki_Generate_Answer",
    "hotpot_qa_distractor_Generate_Answer",
    "hotpot_qa_distractor_Generate_Title_2",
    "hotpot_qa_fullwiki_Generate_Title_2",
    "hotpot_qa_fullwiki_Generate_Title_1",
    "hotpot_qa_distractor_Generate_Title_1",
    "hotpot_qa_distractor_Generate_Question",
    "hotpot_qa_fullwiki_Generate_Question",
    "tab_fact_tab_fact_tab_fact_3",
    "tab_fact_tab_fact_tab_fact_2",
    "tab_fact_tab_fact_tab_fact_1",
    "tab_fact_tab_fact_tab_fact_7",
    "tab_fact_tab_fact_tab_fact_4",
    "tab_fact_tab_fact_tab_fact_5",
    "tab_fact_tab_fact_tab_fact_6",
    "wiki_hop_masked_Choose_Best_Object_Candidate",
    "wiki_hop_masked_Indirect_Question_about_Birthplace_Citizenship_Place_of_Death",
    "narrativeqa_Template_05",
    "ecthr_cases_alleged_violation_prediction_silver_rationales",
    # Tasks with broken cached files
    "gigaword_summarize_",
]

seqio.MixtureRegistry.add(
    "all_tasks_combined_max_1m",
    [task for task in seqio.TaskRegistry.names() if task not in TASK_BLACKLIST],
    default_rate=functools.partial(seqio.mixing_rate_num_examples, maximum=1000000),
)

seqio.MixtureRegistry.add(
    "all_super_glue_tasks",
    [task for task in seqio.TaskRegistry.names() if task.startswith("super_glue")],
    default_rate=seqio.mixing_rate_num_examples,
)

# Tasks deemed as clean/useful
# with open("dataset_subset_template.csv") as in_file:  # File not found in package data
#     reader = csv.DictReader(in_file)
#     all_tasks = [row for row in reader]

# safe_creteria = [
#     "template_bug",
#     "negated_answers",
#     "counting",
#     "answer_span_indices",
#     "non_natural_language",
#     "generative_non_true_implausible",
# ]

# aggressive_creteria = [
#     "generative_non_true_task",
#     "nontrivial_choices_hidden",
#     "awkward_phrasing",
#     "ungrammatical",
# ] + safe_creteria


# def clean(prompt):
#     for criterion in safe_creteria:  # or aggressive_creteria
#         if prompt.get(criterion):
#             return False
#     return True


# tasks = list(filter(clean, all_tasks))
# CLEAN_TASKS = [t for t in tasks if not t["skip_train"]]


CLEAN_TASKS = [
    "adversarial_qa_dbert_adversarial_qa_dbert_1",
    "adversarial_qa_dbert_adversarial_qa_dbert_2",
    "adversarial_qa_dbert_adversarial_qa_dbert_3",
    "adversarial_qa_dbert_adversarial_qa_dbert_4",
    "adversarial_qa_dbert_adversarial_qa_dbert_5",
    "adversarial_qa_dbert_adversarial_qa_dbert_6",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_1",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_2",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_3",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_4",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_5",
    "adversarial_qa_dbidaf_adversarial_qa_dbidaf_6",
    "adversarial_qa_droberta_adversarial_qa_droberta_1",
    "adversarial_qa_droberta_adversarial_qa_droberta_2",
    "adversarial_qa_droberta_adversarial_qa_droberta_3",
    "adversarial_qa_droberta_adversarial_qa_droberta_4",
    "adversarial_qa_droberta_adversarial_qa_droberta_5",
    "adversarial_qa_droberta_adversarial_qa_droberta_6",
    "ag_news_classify",
    "ag_news_classify_with_choices",
    "ag_news_recommend",
    "ag_news_which_section",
    "ag_news_which_section_choices",
    "amazon_polarity_Template_1",
    "amazon_polarity_Template_2",
    "amazon_polarity_Template_3",
    "amazon_polarity_Template_4",
    "amazon_polarity_Template_5",
    "amazon_polarity_Template_6",
    "app_reviews_categorize_rating_using_review",
    "app_reviews_convert_to_rating",
    "app_reviews_convert_to_star_rating",
    "ai2_arc_ARC_Challenge_qa_options",
    "ai2_arc_ARC_Challenge_test",
    "ai2_arc_ARC_Easy_qa_options",
    "ai2_arc_ARC_Easy_test",
    "circa_goldstandard1_judgement",
    "circa_goldstandard2_judgement",
    "circa_judgement",
    "circa_possible_qn",
    "circa_question_declarative",
    "cnn_dailymail_3.0.0_generate_story",
    "cnn_dailymail_3.0.0_news_card_view",
    "cnn_dailymail_3.0.0_news_stock",
    "cnn_dailymail_3.0.0_news_summary",
    "cnn_dailymail_3.0.0_spice_up_story",
    "codah_codah_answer_no_option",
    "codah_codah_answer_with_option",
    "codah_codah_answer_with_option_idx",
    "codah_codah_answer_with_option_post",
    "codah_codah_choose_from_list",
    "codah_codah_finish_from_the_list",
    "codah_codah_finish_from_the_list_post",
    "codah_codah_finish_pre",
    "codah_codah_question_category",
    "codah_codah_question_category_bis",
    "common_gen_Example_prompt",
    "common_gen_Given_concepts",
    "common_gen_Put_together",
    "common_gen_choice_in_concept_centric_sentence_generation",
    "common_gen_sentence_to_concepts",
    "cos_e_v1.11_description_question_option_id",
    "cos_e_v1.11_description_question_option_text",
    "cos_e_v1.11_generate_explanation_no_given_answer",
    "cos_e_v1.11_question_description_option_id",
    "cos_e_v1.11_question_description_option_text",
    "cos_e_v1.11_question_option_description_id",
    "cos_e_v1.11_question_option_description_text",
    "cosmos_qa_context_description_question_answer_id",
    "cosmos_qa_context_description_question_answer_text",
    "cosmos_qa_context_description_question_text",
    "cosmos_qa_context_question_answer_description_id",
    "cosmos_qa_context_question_answer_description_text",
    "cosmos_qa_context_question_description_answer_id",
    "cosmos_qa_context_question_description_answer_text",
    "cosmos_qa_context_question_description_text",
    "cosmos_qa_description_context_question_answer_id",
    "cosmos_qa_description_context_question_answer_text",
    "cosmos_qa_description_context_question_text",
    "cosmos_qa_no_prompt_id",
    "cosmos_qa_no_prompt_text",
    "dbpedia_14_dbpedia_1",
    "dbpedia_14_dbpedia_10",
    "dbpedia_14_dbpedia_3",
    "dbpedia_14_dbpedia_5",
    "dbpedia_14_dbpedia_7",
    "dbpedia_14_dbpedia_8",
    "dbpedia_14_dbpedia_9",
    "dream_answer_to_dialogue",
    "dream_baseline",
    "dream_conversation",
    "dream_generate_first_utterance",
    "dream_generate_last_utterance",
    "emo_feeling",
    "emo_final_message",
    "emo_persons_describe",
    "emo_persons_infer",
    "emo_spoke_last",
    "freebase_qa_inference_chain_prompt",
    "freebase_qa_inference_chain_prompt_context",
    "freebase_qa_qa_context_1",
    "freebase_qa_qa_context_2",
    "freebase_qa_qa_template_basic",
    "gigaword_Document_",
    "gigaword_Summarize_this_document_",
    "gigaword_TLDR",
    "gigaword_generate_summary_for_this",
    "gigaword_in_a_nutshell",
    "gigaword_reverse_writing",
    "gigaword_reverse_writing_2",
    "gigaword_summarize_",
    "gigaword_write_one_sentence",
    "glue_mrpc_equivalent",
    "glue_mrpc_paraphrase",
    "glue_mrpc_replace",
    "glue_mrpc_same_thing",
    "glue_mrpc_want_to_know",
    "glue_qqp_answer",
    "glue_qqp_duplicate",
    "glue_qqp_duplicate_or_not",
    "glue_qqp_quora",
    "glue_qqp_same_thing",
    "glue_sst2_following_positive_negative",
    "glue_sst2_happy_or_mad",
    "glue_sst2_positive_negative_after",
    "glue_sst2_review",
    "glue_sst2_said",
    "hellaswag_YesNo_0",
    "hellaswag_YesNo_1",
    "hellaswag_YesNo_2",
    "hellaswag_YesNo_3",
    "hellaswag_YesNo_reversed_0",
    "hellaswag_YesNo_reversed_1",
    "hellaswag_YesNo_reversed_2",
    "hellaswag_YesNo_reversed_3",
    "hellaswag_complete_first_then",
    "hellaswag_first_then",
    "hellaswag_how_ends",
    "hellaswag_if_begins_how_continues",
    "hellaswag_which_ending",
    "imdb_imdb_1",
    "imdb_imdb_3",
    "imdb_imdb_4",
    "imdb_imdb_5",
    "imdb_imdb_6",
    "imdb_imdb_7",
    "imdb_imdb_8",
    "imdb_imdb_9",
    "mc_taco_mc_taco_1",
    "mc_taco_mc_taco_2",
    "mc_taco_mc_taco_3",
    "mc_taco_mc_taco_4",
    "mc_taco_mc_taco_5",
    "mc_taco_mc_taco_6",
    "onestop_english_ara_context",
    "onestop_english_assess",
    "onestop_english_ats",
    "onestop_english_esl_context",
    "onestop_english_esl_variation",
    "openbookqa_main_choices",
    "openbookqa_main_choose_an_answer_with_options",
    "openbookqa_main_only_options",
    "openbookqa_main_pick_answer_with_options",
    "openbookqa_main_pick_using_id",
    "openbookqa_main_which_correct",
    "paws_labeled_final_PAWS_ANLI_GPT3",
    "paws_labeled_final_PAWS_ANLI_GPT3_no_label",
    "piqa_Correct_the_solution",
    "piqa_Correct_the_solution_if_false_from_sol_1",
    "piqa_Correct_the_solution_if_false_from_sol_2",
    "piqa_Does_this_solution_make_sense_sol1",
    "piqa_Does_this_solution_make_sense_sol2",
    "piqa_Generate_a_similar_but_wrong_solution",
    "piqa_choose_the_most_appropriate_solution",
    "piqa_no_prompt_needed",
    "qa_srl_context_answer",
    "qa_srl_context_qn",
    "qa_srl_predicate",
    "qa_srl_qa",
    "qasc_is_correct_0",
    "qasc_is_correct_1",
    "qasc_qu_combined",
    "qasc_sep_combined_can_tell",
    "qasc_sep_qu",
    "quail_context_description_question_answer_id",
    "quail_context_description_question_answer_text",
    "quail_context_description_question_text",
    "quail_context_question_answer_description_id",
    "quail_context_question_answer_description_text",
    "quail_context_question_description_answer_id",
    "quail_context_question_description_answer_text",
    "quail_context_question_description_text",
    "quail_description_context_question_answer_id",
    "quail_description_context_question_answer_text",
    "quail_description_context_question_text",
    "quail_no_prompt_id",
    "quail_no_prompt_text",
    "quartz_para_question_1",
    "quartz_para_question_2",
    "quartz_para_question_3_choices",
    "quartz_para_question_4_choices",
    "quartz_para_question_plain",
    "quartz_question_para_1",
    "quartz_question_para_2",
    "quartz_question_para_3",
    "quoref_Template_1",
    "quoref_Template_2",
    "quoref_Template_3",
    "quoref_Template_5",
    "race_high_Read_the_article_and_answer_the_question_no_option_",
    "race_high_Read_the_article_and_select_the_best_answer",
    "race_high_Write_a_multi_choice_question_for_the_following_article",
    "race_high_Write_a_multi_choice_question_for_the_following_article_2",
    "race_middle_Read_the_article_and_answer_the_question_no_option_",
    "race_middle_Read_the_article_and_select_the_best_answer",
    "race_middle_Write_a_multi_choice_question_for_the_following_article",
    "race_middle_Write_a_multi_choice_question_for_the_following_article_2",
    "ropes_funky_prompt",
    "ropes_plain",
    "ropes_plain_bottom_hint",
    "ropes_plain_no_background",
    "ropes_prompt_beginning",
    "ropes_prompt_bottom_hint_beginning",
    "ropes_prompt_bottom_no_hint",
    "ropes_prompt_mix",
    "rotten_tomatoes_rt_1",
    "rotten_tomatoes_rt_10",
    "rotten_tomatoes_rt_2",
    "rotten_tomatoes_rt_3",
    "rotten_tomatoes_rt_4",
    "rotten_tomatoes_rt_5",
    "rotten_tomatoes_rt_6",
    "rotten_tomatoes_rt_7",
    "rotten_tomatoes_rt_8",
    "rotten_tomatoes_rt_9",
    "social_i_qa_social_i_qa1",
    "social_i_qa_social_i_qa2",
    "social_i_qa_social_i_qa3",
    "social_i_qa_social_i_qa4",
    "social_i_qa_social_i_qa5",
    "squad_v2_Jeopardy_with_Context",
    "squad_v2_Jeopardy_without_Context",
    "squad_v2_Questions_with_Context",
    "squad_v2_Questions_with_Context_Without_Prompt_Keywords",
    "squad_v2_Topic_Prediction_Context",
    "squad_v2_Topic_Prediction_Context_with_randomized_prompt_options",
    "squad_v2_Topic_Prediction_Context_with_randomized_prompt_options_placed_in_the_end",
    "squad_v2_Topic_Prediction_Question_and_Answer_Pair",
    "squad_v2_Trivia",
    "super_glue_boolq_GPT_3_Style",
    "super_glue_boolq_I_wonder_",
    "super_glue_boolq_based_on_the_following_passage",
    "super_glue_boolq_based_on_the_previous_passage",
    "super_glue_boolq_could_you_tell_me_",
    "super_glue_copa_C1_or_C2_premise_so_because_",
    "super_glue_copa__As_a_result_C1_or_C2_",
    "super_glue_copa__What_could_happen_next_C1_or_C2_",
    "super_glue_copa__which_may_be_caused_by",
    "super_glue_copa__which_may_cause_C1_or_C2_",
    "super_glue_copa__why_C1_or_C2",
    "super_glue_multirc_I_was_going_to_say_",
    "super_glue_multirc_Would_it_be_good_to_answer_",
    "super_glue_multirc_is_a_correct_answer_",
    "super_glue_multirc_is_the_correct_answer_",
    "super_glue_multirc_paragraph_question_is_it_",
    "super_glue_record_Can_you_figure_out_",
    "super_glue_record_In_the_question_above_the_placeholder_stands_for",
    "super_glue_record_What_could_the_placeholder_be_",
    "super_glue_record_Which_one_is_the_placeholder_",
    "super_glue_record_the_placeholder_refers_to_",
    "super_glue_wic_GPT_3_prompt",
    "super_glue_wic_GPT_3_prompt_with_label",
    "super_glue_wic_question_context",
    "super_glue_wic_question_context_meaning",
    "super_glue_wic_question_context_meaning_with_label",
    "super_glue_wic_similar_sense",
    "super_glue_wsc.fixed_Here_p_stands_for_",
    "super_glue_wsc.fixed_In_the_previous_sentence_the_pronoun_refers_to_",
    "super_glue_wsc.fixed_Who_is_are_",
    "super_glue_wsc.fixed_in_the_passage_above_the_pronoun_X_refers_to_",
    "super_glue_wsc.fixed_passage_what_does_the_pronoun_refer_to_",
    "swag_regular_YesNo_0",
    "swag_regular_YesNo_1",
    "swag_regular_YesNo_2",
    "swag_regular_YesNo_3",
    "swag_regular_YesNo_reversed_0",
    "swag_regular_YesNo_reversed_1",
    "swag_regular_YesNo_reversed_2",
    "swag_regular_YesNo_reversed_3",
    "swag_regular_complete_first_then",
    "swag_regular_first_then",
    "swag_regular_how_ends",
    "swag_regular_if_begins_how_continues",
    "swag_regular_which_ending",
    "trec_fine_grained_ABBR",
    "trec_fine_grained_ABBR_context_first",
    "trec_fine_grained_DESC",
    "trec_fine_grained_DESC_context_first",
    "trec_fine_grained_ENTY",
    "trec_fine_grained_ENTY_context_first",
    "trec_fine_grained_HUM",
    "trec_fine_grained_HUM_context_first",
    "trec_fine_grained_LOC",
    "trec_fine_grained_LOC_context_first",
    "trec_fine_grained_NUM",
    "trec_fine_grained_NUM_context_first",
    "trec_fine_grained_open",
    "trec_fine_grained_open_context_first",
    "trec_trec1",
    "trec_trec2",
    "trivia_qa_rc_context_self_description",
    "trivia_qa_rc_question_answer",
    "trivia_qa_rc_question_with_instruction",
    "trivia_qa_rc_reading_comprehension_1",
    "trivia_qa_rc_reading_comprehension_2",
    "web_questions_credible_question",
    "web_questions_if_answers_what_question",
    "web_questions_potential_correct_answer",
    "web_questions_question_answer",
    "web_questions_suggest_question",
    "wiki_bio_comprehension",
    "wiki_bio_guess_person",
    "wiki_bio_key_content",
    "wiki_bio_what_content",
    "wiki_bio_who",
    "wiki_hop_original_Explain_Relation",
    "wiqa_effect_with_label_answer",
    "wiqa_effect_with_string_answer",
    "wiqa_impacting_the_process",
    "wiqa_question_type",
    "wiqa_remove_first_step",
    "wiqa_remove_first_step_bis",
    "wiqa_remove_last_step",
    "wiqa_remove_last_step_bis",
    "xsum_Document_",
    "xsum_Summarize_this_document_",
    "xsum_TLDR",
    "xsum_generate_summary_for_this",
    "xsum_summarize_",
    "xsum_write_one_sentence",
    "yelp_review_full_based_on_that",
    "yelp_review_full_format_rating",
    "yelp_review_full_format_score",
    "yelp_review_full_format_star",
    "yelp_review_full_on_a_scale",
    "yelp_review_full_so_i_would",
    "yelp_review_full_this_place",
]

seqio.MixtureRegistry.add(
    "clean_tasks",
    [task for task in CLEAN_TASKS if task not in TASK_BLACKLIST],
    default_rate=functools.partial(seqio.mixing_rate_num_examples, maximum=1000000),
)

# Tasks to evaluate models trained on clean_tasks
# CLEAN_EVAL_TASKS = [t for t in tasks if t["do_eval"]]


CLEAN_EVAL_TASKS = [
    "anli_GPT_3_style",
    "anli_based_on_the_previous_passage",
    "anli_does_S1_entail_S2_",
    "anli_given_does_it_follow_that_",
    "anli_given_it_must_be_true_that_",
    "circa_goldstandard1_judgement",
    "circa_goldstandard2_judgement",
    "emo_feeling",
    "emo_final_message",
    "emo_persons_describe",
    "emo_persons_infer",
    "emo_spoke_last",
    "glue_cola_Following_sentence_acceptable",
    "glue_cola_Make_sense_yes_no",
    "glue_cola_Previous_sentence_acceptable",
    "glue_cola_editing",
    "glue_cola_jinja_example",
    "glue_mrpc_equivalent",
    "glue_mrpc_paraphrase",
    "glue_mrpc_replace",
    "glue_mrpc_same_thing",
    "glue_mrpc_want_to_know",
    "hans_GPT_3_style",
    "hans_Suppose_Can_we_infer_that_",
    "hans_based_on_the_previous_passage",
    "hans_does_S1_entail_S2_",
    "hans_given_does_it_follow_that_",
    "hans__does_the_previous_passage_support_the_claim_that",
    "mc_taco_mc_taco_1",
    "mc_taco_mc_taco_3",
    "nq_open_context_self_description",
    "nq_open_question_answer",
    "nq_open_question_with_instruction",
    "openbookqa_main_choices",
    "openbookqa_main_choose_an_answer_with_options",
    "openbookqa_main_only_options",
    "openbookqa_main_pick_answer_with_options",
    "openbookqa_main_pick_using_id",
    "openbookqa_main_which_correct",
    "qa_srl_qa",
    "race_high_Read_the_article_and_select_the_best_answer",
    "race_middle_Read_the_article_and_select_the_best_answer",
    "social_i_qa_social_i_qa1",
    "social_i_qa_social_i_qa3",
    "super_glue_boolq_GPT_3_Style",
    "super_glue_boolq_I_wonder_",
    "super_glue_boolq_based_on_the_following_passage",
    "super_glue_boolq_based_on_the_previous_passage",
    "super_glue_boolq_could_you_tell_me_",
    "super_glue_cb_GPT_3_style",
    "super_glue_cb_based_on_the_previous_passage",
    "super_glue_cb_does_S1_contradict_S2_",
    "super_glue_cb_does_S1_entail_S2_",
    "super_glue_cb_given_does_it_follow_that_",
    "super_glue_cb_given_it_must_be_true_that_",
    "super_glue_copa_C1_or_C2_premise_so_because_",
    "super_glue_copa__As_a_result_C1_or_C2_",
    "super_glue_copa__What_could_happen_next_C1_or_C2_",
    "super_glue_copa__which_may_be_caused_by",
    "super_glue_copa__which_may_cause_C1_or_C2_",
    "super_glue_copa__why_C1_or_C2",
    "super_glue_multirc_I_was_going_to_say_",
    "super_glue_multirc_Would_it_be_good_to_answer_",
    "super_glue_multirc_is_a_correct_answer_",
    "super_glue_multirc_is_the_correct_answer_",
    "super_glue_multirc_paragraph_question_is_it_",
    "super_glue_record_Can_you_figure_out_",
    "super_glue_record_In_the_question_above_the_placeholder_stands_for",
    "super_glue_record_What_could_the_placeholder_be_",
    "super_glue_record_Which_one_is_the_placeholder_",
    "super_glue_record_the_placeholder_refers_to_",
    "super_glue_rte_GPT_3_style",
    "super_glue_rte_Suppose_Can_we_infer_that_",
    "super_glue_rte_based_on_the_previous_passage",
    "super_glue_rte_does_S1_entail_S2_",
    "super_glue_rte_given_does_it_follow_that_",
    "super_glue_rte__Therefore_we_re_licensed_to_say_that_",
    "super_glue_rte__does_the_previous_passage_support_the_claim_that",
    "super_glue_wic_GPT_3_prompt",
    "super_glue_wic_GPT_3_prompt_with_label",
    "super_glue_wic_question_context",
    "super_glue_wic_question_context_meaning",
    "super_glue_wic_question_context_meaning_with_label",
    "super_glue_wic_similar_sense",
    "super_glue_wsc.fixed_Here_p_stands_for_",
    "super_glue_wsc.fixed_In_the_previous_sentence_the_pronoun_refers_to_",
    "super_glue_wsc.fixed_Who_is_are_",
    "super_glue_wsc.fixed_in_the_passage_above_the_pronoun_X_refers_to_",
    "super_glue_wsc.fixed_passage_what_does_the_pronoun_refer_to_",
    "xsum_Document_",
    "xsum_Summarize_this_document_",
    "xsum_TLDR",
    "xsum_generate_summary_for_this",
    "xsum_summarize_",
    "xsum_write_one_sentence",
]


seqio.MixtureRegistry.add(
    "clean_eval_tasks",
    [task for task in CLEAN_EVAL_TASKS if task not in TASK_BLACKLIST],
    default_rate=functools.partial(seqio.mixing_rate_num_examples, maximum=1000000),
)
